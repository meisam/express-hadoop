{"name":"Express-hadoop","google":"","note":"Don't delete this file! It's used internally to help with page regeneration.","tagline":"A Hadoop extension to support efficient high dimensional array query and analysis","body":"#Introduction#\r\nEXPRESS is proposed to enable efficient processing of high-dimensional scientific data. It takes advantage of prior knowledge in **data structure** and **data usage pattern**. By performing **incongruent data partitioning** and **locality aware task scheduling**, EXPRESS effectively reduces the network traffic and task execution time. Highlighted features involve:\r\n\r\n* User interfaces to describe and use data in a **structure-aware** language. \r\n* A novel **incongruent data partitioning** scheme for replicas. EXPRESS supports the coexistence of multiple data partitioning for the same data. It also introduces a set of optimizations to fully realize the potential of incongruent partitioning.\r\n* Data **layout aware** task selection and scheduling. By exposing data layout information, the EXPRESS scheduler collocates the map/reduce tasks with related data. When data layout matches its usage pattern, EXPRESS can select the proper map task to accelerate the data loading.\r\n\r\n#Installation#\r\n##Prerequirement  \r\n**Packages** hadoop-1.0.1, ant, patch\r\n\r\n**Environmental Variables** HADOOP_HOME\r\n\r\n##Steps\r\n1. [Apply express patch to hadoop-1.0.1](http://wiki.apache.org/hadoop/HowToContribute)\r\n\r\n    ``cd $HADOOP_HOME && patch -p0 < $EXPRESS_HOME/express-hadoop-1.0.1.patch``\r\n\r\n2. Create express-hadoop.jar\r\n\r\n    ``ant -f build.xml jar``\r\n  \r\n3. Edit ${EXPRESS_ROOT}/test/env.sh to set the path of Hadoop and Express \r\n4. Recompile hadoop-1.0.1\r\n\r\n    ``ant -f build.xml compile``\r\n\r\n#How To#\r\n* Use express.hdd.HDFGen to generate test data with specifc partitioning scheme\r\n    \r\n    ``bin/hadoop jar express-hadoop.jar hdf.test.HDFGen [dataSize] [partitionOffset] [recordSize] [partitionSize] [outDir]``\r\n\r\n* Use express.hdd.HDFMicroBenchmark to load data with specific pattern\r\n\r\n    ``bin/hadoop jar express-hadoop.jar hdf.test.HDFMicroBenchmark [dataSize] [chunkOffset] [chunkSize] [inDir] [outDir]``\r\n\r\n* run tests/validate.sh for validation\r\n\r\n#A Motivating Case#\r\n**Hyperspectral data** is usually collected by sensors on an airborne or spaceborne platform. It is a valuable data source for many critical applications, such as mineral exploration, agricultural assessment, and special target recognition. Figure below shows a representative image of a hyperspectral cube. \r\n\r\n<figure>\r\n  <img src=\"http://upload.wikimedia.org/wikipedia/en/4/48/HyperspectralCube.jpg\" title=\"Graphic representation of hyperspectral data\" alt=\"Graphic representation of hyperspectral data\" height=\"160\" width=\"160\" />\r\n  \r\n  <br><figcaption><b>Figure 1</b> Graphic representation of hyperspectral data</figcaption>\r\n</figure>\r\n\r\nThe image consists of two spatial dimensions and one spectral dimension. Terabytes of such data have been produced daily by EOS satellites since 1997. The accumulation of global hyperspectral datasets now reaches the petabytes scale. \r\n\r\nTo analyze the data for a special purpose like geometric correction or mineral searching, the data needs to be partitioned regularly as the top cube shown in below Figure (a). The partitions then can be processed independently. MapReduce seems the proper solution at first, but two issues are readily apparent:\r\n\r\n<figure>\r\n  <a href=\"https://picasaweb.google.com/lh/photo/xvx5i6rLQwl2BNaZ4ps5pNMTjNZETYmyPJy0liipFm0?feat=embedwebsite\"><img src=\"https://lh5.googleusercontent.com/-KE6-S-6Jq6M/T9Jo0BKGbbI/AAAAAAAAAAk/KlekTZmfBmE/s640/mot.png\" title=\"Data Usage and Storage Partitioning\" height=\"514\" width=\"640\" /></a>\r\n  \r\n  <br><figcaption><b>Figure 2</b> Data Usage and Storage Partitioning</figcaption>\r\n</figure>\r\n\r\n1. In traditional MapReduce, the data partition and distribution are not directly controlled by the user. So when the data usage pattern is illustrated by the top cube in Figure 2(b), the data may be actually partitioned as cubes in Figure 2(a).\r\n \r\n2. Various usage patterns (partitioning) could be applied to the same chunk of data, depending on the analysis being performed. For instance, change detection tasks require broad spatial regions, and several adjacent spectral layers; signal processing tasks have no spatial region requirement, but a partition needs to contain all the spectral layers for one pixel. Figure 2(b) gives three possible usage patterns.\r\n\r\nThe storage-usage mismatch in Figure 2(a) and Figure 2(b) causes extra network traffic and synchronization. Figure 2(d) shows that in order to collect the red chunk of data for processing, nine blocks are accessed. Since data blocks are distributed over all nodes in the system, network latency variance and maximum bandwidth limitations could greatly slow down this data access. Due to the absence of data locality, the scalability of the map task stage degrades enormously in the scenario represented by Figure 2(d). When storage matches the data usage as described in Figure 2(e), data locality is preserved and the system becomes scalable again.\r\n\r\n#Features#\r\n**Incongruent Partition** enables different partition for each replica of the same data.\r\n\r\n\r\n**Locality Aware Reducer Scheduling** takes into account the data produced by the mapper and its locality. Task scheduler therefore makes decision to minimize the data movement between mappers and reducers over network.\r\n\r\n\r\n**PipeFile** Unlike pipelines in Hadoop which enable data streaming to external local program, pipeFile is a powerful solution to connect two or more MapReduce jobs. It borrows the idea from Unix named pipe, while apply it into a distributed system.\r\n\r\n#People#\r\n* Developer: [Siyuan Ma](http://siyuan.biz)\r\n* Faculty: [Xian-He Sun](http://www.cs.iit.edu/~scs/sun/), \r\n           [Robert Ross](http://www.mcs.anl.gov/~rross/)\r\n\r\n"}